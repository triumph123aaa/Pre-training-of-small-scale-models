# Pre-training-of-small-scale-models
I tried to train a 0.03B pre-trained model using 31GB of text data and successfully experienced the entire process of pre-training a language model.
